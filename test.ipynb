{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import face_recognition\n",
    "import cv2, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df97c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\P2E_S5\\P2E_S5_C1\\P2E_S5_C1.1\"\n",
    "output_dir =r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\extracted_faces\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff8734",
   "metadata": {},
   "source": [
    "# Extracting Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_log = os.path.join(output_dir, \"empty_images_c1.txt\")\n",
    "empty_images = []\n",
    "haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "for img_name in tqdm(os.listdir(dataset_path), desc=\"Extracting Faces\"):\n",
    "    if not img_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(dataset_path, img_name)\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        empty_images.append(img_name)\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    rgb = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    face_locations = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "\n",
    "    if not face_locations:\n",
    "        faces_haar = haar_cascade.detectMultiScale(gray, 1.1, 3, minSize=(20, 20))\n",
    "        face_locations = [(y, x+w, y+h, x) for (x, y, w, h) in faces_haar]\n",
    "\n",
    "    if not face_locations:\n",
    "        empty_images.append(img_name)\n",
    "        continue\n",
    "\n",
    "    for i, (top, right, bottom, left) in enumerate(face_locations):\n",
    "        face = image[top:bottom, left:right]\n",
    "        cv2.imwrite(os.path.join(output_dir, f\"{os.path.splitext(img_name)[0]}_f{i+1}.jpg\"), face)\n",
    "\n",
    "with open(empty_log, 'w') as f:\n",
    "    f.write('\\n'.join(empty_images))\n",
    "\n",
    "print(f\"‚úÖ Done! Extracted faces saved in: {output_dir}\")\n",
    "print(f\"‚ùå No-face images: {len(empty_images)} logged in {empty_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f329e9",
   "metadata": {},
   "source": [
    "## Group By Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ece311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "faces_path = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\extracted_faces\"\n",
    "output_path = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\clustered_faces\"\n",
    "employee_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\employee_images\"  \n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(employee_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a51196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import os, face_recognition, shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "faces_path = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\extracted_faces\"\n",
    "output_path = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\clustered_faces\"\n",
    "employee_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\employee_images\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(employee_dir, exist_ok=True)\n",
    "\n",
    "encodings, filenames = [], []\n",
    "\n",
    "# ---- Step 1: Encode ----\n",
    "for img_name in tqdm(os.listdir(faces_path), desc=\"Encoding faces\"):\n",
    "    if not img_name.lower().endswith(('.jpg', '.png')): continue\n",
    "    path = os.path.join(faces_path, img_name)\n",
    "    img = face_recognition.load_image_file(path)\n",
    "    enc = face_recognition.face_encodings(img)\n",
    "    if enc:\n",
    "        encodings.append(enc[0])\n",
    "        filenames.append(path)\n",
    "\n",
    "encodings = np.array(encodings)\n",
    "print(f\"‚úÖ Encoded {len(encodings)} faces\")\n",
    "\n",
    "# ---- Step 2: Try smaller eps ----\n",
    "# You can experiment with 0.4, 0.35, or 0.3 depending on data diversity\n",
    "clt = DBSCAN(eps=0.38, min_samples=3, metric=\"euclidean\").fit(encodings)\n",
    "labels = clt.labels_\n",
    "\n",
    "unique_labels = sorted(set(labels) - {-1})\n",
    "print(f\"üß† Found {len(unique_labels)} distinct people\")\n",
    "print(f\"üóëÔ∏è Noise faces: {(labels == -1).sum()}\")\n",
    "\n",
    "# ---- Step 3: Save ----\n",
    "for label in unique_labels:\n",
    "    cluster_dir = os.path.join(output_path, f\"person_{label}\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "    cluster_files = [f for f, l in zip(filenames, labels) if l == label]\n",
    "    \n",
    "    # copy only 2-3 samples to keep small\n",
    "    for f in cluster_files[:3]:\n",
    "        shutil.copy(f, cluster_dir)\n",
    "\n",
    "    # representative image\n",
    "    shutil.copy(cluster_files[0], os.path.join(employee_dir, f\"Employee_{label+1}.jpg\"))\n",
    "\n",
    "print(\"‚úÖ Done clustering!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\employee_images\"\n",
    "employee_files = [f for f in os.listdir(employee_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "print(f\"Found {len(employee_files)} employee images:\")\n",
    "print(employee_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db58e8",
   "metadata": {},
   "source": [
    "# Checking Camera 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Load employee faces\n",
    "employee_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\employee_images\"\n",
    "if not os.path.exists(employee_dir):\n",
    "    os.makedirs(employee_dir)\n",
    "    print(f\"Created {employee_dir} - Add face images here!\")\n",
    "    exit()\n",
    "    \n",
    "images = []\n",
    "classNames = []\n",
    "for img_name in os.listdir(employee_dir):\n",
    "    if img_name.lower().endswith(('.jpg', '.png')):\n",
    "        img = cv2.imread(os.path.join(employee_dir, img_name))\n",
    "        if img is None:\n",
    "            print(f\"Failed to load {img_name}\")\n",
    "            continue\n",
    "        images.append(img)\n",
    "        classNames.append(os.path.splitext(img_name)[0])\n",
    "        \n",
    "def findEncodings(images):\n",
    "    encodeList = []\n",
    "    for img_name, img in zip(os.listdir(employee_dir), images):\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encodes = face_recognition.face_encodings(img_rgb)\n",
    "        if encodes:\n",
    "            encodeList.append(encodes[0])\n",
    "        else:\n",
    "            print(f\"No encoding for {img_name}\")\n",
    "    return encodeList\n",
    "\n",
    "encoded_face_train = findEncodings(images)\n",
    "print(f\"Encoded {len(encoded_face_train)} known faces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e669eb",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17413d39",
   "metadata": {},
   "source": [
    "### for the full body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using CPU for inference.\n",
      "‚úÖ Encoded 28 known faces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fast C3 Frames:  16%|‚ñà‚ñã        | 131/806 [00:01<00:03, 171.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_2 recognized in 00000176.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fast C3 Frames:  22%|‚ñà‚ñà‚ñè       | 178/806 [00:03<00:22, 28.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_1 recognized in 00000206.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fast C3 Frames:  24%|‚ñà‚ñà‚ñç       | 192/806 [00:04<00:30, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_18 recognized in 00000236.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fast C3 Frames:  25%|‚ñà‚ñà‚ñç       | 200/806 [00:06<00:19, 31.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_20 recognized in 00000236.jpg\n",
      "‚ö° Avg FPS: 127.38\n",
      "‚úÖ Done! Fast output saved to: C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\P2E_S5\\P2E_S5_C3\\P2E_S5_C3.1\\boxed_output_fast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# import cv2, os, numpy as np, face_recognition, torch\n",
    "# from ultralytics import YOLO\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# # ---------------- YOLO Model ----------------\n",
    "# model_path = \"yolov8n-face.pt\" if os.path.exists(\"yolov8n-face.pt\") else \"yolov8n.pt\"\n",
    "# model = YOLO(model_path)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "# print(f\"‚úÖ Using {device.upper()} for inference.\")\n",
    "\n",
    "# # ---------------- Load Known Faces ----------------\n",
    "# employee_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\employee_images\"\n",
    "# images, classNames = [], []\n",
    "# for img_name in os.listdir(employee_dir):\n",
    "#     if img_name.lower().endswith(('.jpg', '.png')):\n",
    "#         path = os.path.join(employee_dir, img_name)\n",
    "#         img = cv2.imread(path)\n",
    "#         if img is not None:\n",
    "#             images.append(img)\n",
    "#             classNames.append(os.path.splitext(img_name)[0])\n",
    "\n",
    "# def findEncodings(imgs):\n",
    "#     encs = []\n",
    "#     for img in imgs:\n",
    "#         rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         e = face_recognition.face_encodings(rgb)\n",
    "#         if e:\n",
    "#             encs.append(e[0])\n",
    "#     return encs\n",
    "\n",
    "# encoded_face_train = findEncodings(images)\n",
    "# print(f\"‚úÖ Encoded {len(encoded_face_train)} known faces.\")\n",
    "\n",
    "# # ---------------- Attendance ----------------\n",
    "# def markAttendance(name):\n",
    "#     with open('Attendance_C3.csv', 'a') as f:\n",
    "#         now = datetime.now()\n",
    "#         f.write(f'\\n{name},{now.strftime(\"%I:%M:%S %p\")},{now.strftime(\"%d-%B-%Y\")}')\n",
    "\n",
    "# # ---------------- Paths ----------------\n",
    "# image_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\P2E_S5\\P2E_S5_C3\\P2E_S5_C3.1\"\n",
    "# boxed_dir = os.path.join(image_dir, \"boxed_output_fast\")\n",
    "# os.makedirs(boxed_dir, exist_ok=True)\n",
    "\n",
    "# processed_faces = set()\n",
    "# image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
    "\n",
    "# # ---------------- Process Frames ----------------\n",
    "# frame_skip = 10\n",
    "# t0 = time.time()\n",
    "\n",
    "# for i, img_file in enumerate(tqdm(image_files, desc=\"Processing Fast C3 Frames\")):\n",
    "#     if i % frame_skip != 0:\n",
    "#         continue  # Skip frames for speed\n",
    "\n",
    "#     path = os.path.join(image_dir, img_file)\n",
    "#     img = cv2.imread(path)\n",
    "#     if img is None:\n",
    "#         continue\n",
    "\n",
    "#     # ‚úÖ YOLO detection (low resolution for speed)\n",
    "#     results = model.predict(img, imgsz=256, conf=0.6, device=device, verbose=False)\n",
    "#     r = results[0]\n",
    "#     boxes = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "\n",
    "#     for (x1, y1, x2, y2) in boxes:\n",
    "#         y1, y2 = max(0, y1+8), min(img.shape[0], y2-8)\n",
    "#         face_crop = img[y1:y2, x1:x2]\n",
    "#         if face_crop.size == 0:\n",
    "#             continue\n",
    "\n",
    "#         rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "#         encs = face_recognition.face_encodings(rgb)\n",
    "#         if not encs:\n",
    "#             continue\n",
    "\n",
    "#         encode = encs[0]\n",
    "#         dists = face_recognition.face_distance(encoded_face_train, encode)\n",
    "#         idx = np.argmin(dists) if len(dists) > 0 else -1\n",
    "\n",
    "#         if idx != -1 and dists[idx] < 0.55:\n",
    "#             name = classNames[idx].upper()\n",
    "#             color = (0, 255, 0)\n",
    "#             if name not in processed_faces:\n",
    "#                 markAttendance(name)\n",
    "#                 processed_faces.add(name)\n",
    "#                 print(f\"‚úÖ {name} recognized in {img_file}\")\n",
    "#         else:\n",
    "#             name = \"UNKNOWN\"\n",
    "#             color = (0, 0, 255)\n",
    "\n",
    "#         cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "#         cv2.putText(img, name, (x1+6, y2-10),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "#     # ‚úÖ Smaller display to speed up\n",
    "#     disp = cv2.resize(img, (480, 360))\n",
    "#     cv2.imshow(\"YOLOv8n-Face (Ultra-Fast)\", disp)\n",
    "#     cv2.imwrite(os.path.join(boxed_dir, f\"boxed_{img_file}\"), img)\n",
    "\n",
    "#     # ‚úÖ Non-blocking & quick refresh\n",
    "#     if cv2.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "# fps = len(image_files) / (time.time() - t0)\n",
    "# print(f\"‚ö° Avg FPS: {fps:.2f}\")\n",
    "# print(\"‚úÖ Done! Fast output saved to:\", boxed_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fb459",
   "metadata": {},
   "source": [
    "### for the faces only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2075fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoded 28 known faces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  17%|‚ñà‚ñã        | 137/806 [00:04<00:26, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_2 recognized in 00000172.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  21%|‚ñà‚ñà        | 169/806 [00:07<00:55, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_19 recognized in 00000204.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  22%|‚ñà‚ñà‚ñè       | 177/806 [00:08<00:59, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_1 recognized in 00000228.jpg\n",
      "‚úÖ EMPLOYEE_6 recognized in 00000228.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  25%|‚ñà‚ñà‚ñç       | 201/806 [00:10<00:52, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_18 recognized in 00000236.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  27%|‚ñà‚ñà‚ñã       | 217/806 [00:12<01:05,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_10 recognized in 00000260.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  29%|‚ñà‚ñà‚ñâ       | 233/806 [00:14<01:11,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_4 recognized in 00000276.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  30%|‚ñà‚ñà‚ñâ       | 241/806 [00:16<01:16,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_3 recognized in 00000284.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  31%|‚ñà‚ñà‚ñà       | 249/806 [00:17<01:20,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_5 recognized in 00000292.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  33%|‚ñà‚ñà‚ñà‚ñé      | 265/806 [00:20<01:21,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_12 recognized in 00000308.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  35%|‚ñà‚ñà‚ñà‚ñç      | 281/806 [00:21<01:06,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_7 recognized in 00000316.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  36%|‚ñà‚ñà‚ñà‚ñå      | 289/806 [00:22<01:03,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_9 recognized in 00000332.jpg\n",
      "‚úÖ EMPLOYEE_14 recognized in 00000332.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  38%|‚ñà‚ñà‚ñà‚ñä      | 305/806 [00:26<01:25,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMPLOYEE_27 recognized in 00000340.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C3 Frames (Hybrid YOLO):  39%|‚ñà‚ñà‚ñà‚ñä      | 312/806 [00:28<00:44, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Avg FPS: 28.74\n",
      "‚úÖ Done! Output saved to: C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\P2E_S5\\P2E_S5_C3\\P2E_S5_C3.1\\boxed_output_fast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2, os, numpy as np, face_recognition, torch, time\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- YOLO Models ----------------\n",
    "face_model_path = \"yolov8n-face-lindevs.pt\"\n",
    "fallback_model_path = \"yolov8n.pt\"\n",
    "\n",
    "face_model = YOLO(face_model_path)\n",
    "fallback_model = YOLO(fallback_model_path)\n",
    "\n",
    "\n",
    "# ---------------- Load Known Faces ----------------\n",
    "employee_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\employee_images\"\n",
    "images, classNames = [], []\n",
    "\n",
    "for img_name in os.listdir(employee_dir):\n",
    "    if img_name.lower().endswith(('.jpg', '.png')):\n",
    "        path = os.path.join(employee_dir, img_name)\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            classNames.append(os.path.splitext(img_name)[0])\n",
    "\n",
    "def findEncodings(imgs):\n",
    "    encs = []\n",
    "    for img in imgs:\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        e = face_recognition.face_encodings(rgb)\n",
    "        if e:\n",
    "            encs.append(e[0])\n",
    "    return encs\n",
    "\n",
    "encoded_face_train = findEncodings(images)\n",
    "print(f\"‚úÖ Encoded {len(encoded_face_train)} known faces.\")\n",
    "\n",
    "# ---------------- Attendance ----------------\n",
    "def markAttendance(name):\n",
    "    with open('Attendance_C3.csv', 'a') as f:\n",
    "        now = datetime.now()\n",
    "        f.write(f'\\n{name},{now.strftime(\"%I:%M:%S %p\")},{now.strftime(\"%d-%B-%Y\")}')\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "image_dir = r\"C:\\Users\\noura\\OneDrive\\Documents\\vid_atten_proj\\P2E_S5\\P2E_S5_C3\\P2E_S5_C3.1\"\n",
    "boxed_dir = os.path.join(image_dir, \"boxed_output_fast\")\n",
    "os.makedirs(boxed_dir, exist_ok=True)\n",
    "\n",
    "processed_faces = set()\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
    "\n",
    "# ---------------- Process Frames ----------------\n",
    "frame_skip = 8  \n",
    "t0 = time.time()\n",
    "\n",
    "for i, img_file in enumerate(tqdm(image_files, desc=\"Processing C3 Frames (Hybrid YOLO)\")):\n",
    "    if i % frame_skip != 0:\n",
    "        continue  # Skip frames for speed\n",
    "\n",
    "    path = os.path.join(image_dir, img_file)\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # ‚úÖ YOLO-FACE detection (higher resolution + lower conf for more recall)\n",
    "    results = face_model.predict(img, imgsz=640, conf=0.8, verbose=False)\n",
    "    r = results[0]\n",
    "    boxes = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "\n",
    "    # ‚úÖ Fallback to normal YOLO if YOLO-Face detects nothing\n",
    "    if len(boxes) == 0:\n",
    "        results = fallback_model.predict(img, imgsz=640, conf=0.8, verbose=False)\n",
    "        r = results[0]\n",
    "        boxes = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2) in boxes:\n",
    "        # ‚úÖ Make box slightly larger for more accurate crops\n",
    "        h, w = img.shape[:2]\n",
    "        pad = 20  # enlarge each side by 20px\n",
    "        x1 = max(0, x1 - pad)\n",
    "        y1 = max(0, y1 - pad)\n",
    "        x2 = min(w, x2 + pad)\n",
    "        y2 = min(h, y2 + pad)\n",
    "\n",
    "        face_crop = img[y1:y2, x1:x2]\n",
    "        if face_crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "        encs = face_recognition.face_encodings(rgb)\n",
    "        if not encs:\n",
    "            continue\n",
    "\n",
    "        encode = encs[0]\n",
    "        dists = face_recognition.face_distance(encoded_face_train, encode)\n",
    "        idx = np.argmin(dists) if len(dists) > 0 else -1\n",
    "\n",
    "        if idx != -1 and dists[idx] < 0.55:\n",
    "            name = classNames[idx].upper()\n",
    "            color = (0, 255, 0)\n",
    "            if name not in processed_faces:\n",
    "                markAttendance(name)\n",
    "                processed_faces.add(name)\n",
    "                print(f\"‚úÖ {name} recognized in {img_file}\")\n",
    "        else:\n",
    "            name = \"UNKNOWN\"\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        # ‚úÖ Bigger and thicker bounding boxes\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 4)  # thickness=4\n",
    "        cv2.putText(img, name, (x1 + 10, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)  # larger font\n",
    "\n",
    "    # ‚úÖ Display smaller window for speed\n",
    "    disp = cv2.resize(img, (480, 360))\n",
    "    cv2.imshow(\"Hybrid YOLOv8 Face Attendance\", disp)\n",
    "    cv2.imwrite(os.path.join(boxed_dir, f\"boxed_{img_file}\"), img)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "fps = len(image_files) / (time.time() - t0)\n",
    "print(f\"‚ö° Avg FPS: {fps:.2f}\")\n",
    "print(\"‚úÖ Done! Output saved to:\", boxed_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
